{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Readme-ai is an automated README file generator powered by large language model APIs. It streamlines documentation creation and maintenance, enhancing developer productivity across all skill levels and domains.</p>"},{"location":"#where-to-start","title":"Where to Start","text":"<ul> <li>Quickstart</li> <li>Usage</li> <li>Features</li> <li>Examples</li> <li>Contributing</li> </ul>"},{"location":"#faq","title":"FAQ","text":"<ul> <li>WIP</li> </ul>"},{"location":"contributing/","title":"Contributing Guidelines","text":"<p>Thanks for your interest in contributing to readme-ai. Please review these guidelines to ensure a smooth process.</p>"},{"location":"contributing/#make-valuable-contributions","title":"Make Valuable Contributions","text":"<p>Strive to make useful, creative, and high quality contributions. This isn't meant to be a high bar, but more of a guiding principle and philosophy. Here's what we mean by these terms:</p> <p>Useful: Solve common problems, use cases, bugs, or new features.</p> <p>Creative: Innovative and helping us all grow and learn new things.</p> <p>High Quality: Well-written, structured, and explained.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<p>To improve and grow the project, we need your help! Here are some ways to get involved:</p> Activity Ideas \ud83d\udc4b Discussions Start a discussion by asking a question or making a suggestion. \ud83d\udc1b Open an Issue Find unhandled exceptions and bugs in the codebase. \ud83d\udcc4 Documentation Write documentation for the project. \ud83e\uddea Testing Write unit tests to increase code coverage. \ud83e\udde9 Feature Requests Brainstorm new ideas such as a CLI option to select any language. \ud83d\udee0\ufe0f Code Contributions Contribute to the codebase and submit a pull request. \ud83d\udd22 Code Readability Find ways to make code more readable and easier to understand. \ud83e\udd14 Other Anything else you can think of! <p>These are just a few examples, and we welcome any other ideas you may have!</p>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Fork the repository and clone it locally.</li> <li>Create a new branch with a descriptive name i.e <code>feature/new-feature-name</code> or <code>bugfix-issue-123</code>.</li> <li>Make focused changes with clear commits.</li> <li>Open a pull request document the changes you've made and why they're necessary.</li> <li>Respond to code reviews from maintainers.</li> </ol>"},{"location":"contributing/#code-quality-expectations","title":"Code Quality Expectations","text":"<ul> <li>Clear, well-documented code</li> <li>Include tests for new code</li> <li>Follow project style standards</li> <li>Rebase onto latest main branch</li> </ul>"},{"location":"contributing/#attribution","title":"Attribution","text":"<p>Contributors to our project will be acknowledged in the project's README.md file.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to our project, you agree to license your contributions under the project's open source license. The project's license can be found in the LICENSE</p> <p>Thank you for your interest in contributing to readme-ai! We appreciate your help and look forward to working with you.</p>"},{"location":"configuration/","title":"Configuration Options","text":"<p>Readme-ai offers a wide range of configuration options to customize your README generation. This page provides a comprehensive list of all available options with detailed explanations.</p>"},{"location":"configuration/#command-line-options","title":"Command-Line Options","text":"Option Description Default Impact <code>--align</code> Text alignment in header <code>center</code> Affects the visual layout of the README header <code>--api</code> LLM API service <code>offline</code> Determines which AI service is used for content generation <code>--badge-color</code> Badge color (name or hex) <code>0080ff</code> Customizes the color of status badges in the README <code>--badge-style</code> Badge icon style type <code>flat</code> Changes the visual style of status badges <code>--base-url</code> Base URL for the repository <code>v1/chat/completions</code> Used for API requests to the chosen LLM service <code>--context-window</code> Max context window of LLM API <code>3999</code> Limits the amount of context provided to the LLM <code>--emojis</code> Add emojis to README sections <code>False</code> Adds visual flair to section headers <code>--header-style</code> Header template style <code>classic</code> Changes the overall look of the README header <code>--image</code> Project logo image <code>blue</code> Sets the main image displayed in the README <code>--model</code> Specific LLM model to use <code>gpt-3.5-turbo</code> Chooses the AI model for content generation <code>--output</code> Output filename <code>readme-ai.md</code> Specifies the name of the generated README file <code>--rate-limit</code> Max API requests per minute <code>5</code> Prevents exceeding API rate limits <code>--repository</code> Repository URL or local path <code>None</code> Specifies the project to analyze <code>--temperature</code> Creativity level for generation <code>0.9</code> Controls the randomness of the AI's output <code>--toc-style</code> Table of contents style <code>bullet</code> Changes the format of the table of contents <code>--top-p</code> Top-p sampling probability <code>0.9</code> Fine-tunes the AI's output diversity <code>--tree-depth</code> Max depth of directory tree <code>2</code> Controls the detail level of the project structure"},{"location":"configuration/#detailed-option-explanations","title":"Detailed Option Explanations","text":""},{"location":"configuration/#-api","title":"--api","text":"<ul> <li>Options: <code>openai</code>, <code>ollama</code>, <code>gemini</code>, <code>offline</code></li> <li>Impact: Determines the AI service used for generating README content. Each service has its own strengths and may produce slightly different results.</li> </ul>"},{"location":"configuration/#-model","title":"--model","text":"<ul> <li>Options vary by API service</li> <li>Impact: Different models have varying capabilities and may produce different quality or style of content. Higher-tier models (e.g., GPT-4) generally produce better results but may be slower or more expensive.</li> </ul>"},{"location":"configuration/#-badge-color","title":"--badge-color","text":"<ul> <li>Options: Any valid color name or hex code</li> <li>Impact: Changes the color of status badges in the README. You can match the color scheme to your project's branding or theme.</li> </ul>"},{"location":"configuration/#-badge-style","title":"--badge-style","text":"<ul> <li>Options: <code>flat</code>, <code>flat-square</code>, <code>for-the-badge</code>, <code>plastic</code>, <code>skills</code>, <code>skills-light</code>, <code>social</code></li> <li>Impact: Changes the visual appearance of status badges in the README. Different styles can better match your project's aesthetic.</li> </ul>"},{"location":"configuration/#-header-style","title":"--header-style","text":"<ul> <li>Options: <code>classic</code>, <code>modern</code>, <code>compact</code></li> <li>Impact: Alters the layout and design of the README header, including how the project title, description, and badges are displayed.</li> </ul>"},{"location":"configuration/#-toc-style","title":"--toc-style","text":"<ul> <li>Options: <code>bullet</code>, <code>fold</code>, <code>links</code>, <code>number</code></li> <li>Impact: Changes how the table of contents is formatted. The <code>fold</code> option creates a collapsible ToC, which can be useful for longer READMEs.</li> </ul>"},{"location":"configuration/#-image","title":"--image","text":"<ul> <li>Options: <code>blue</code>, <code>gradient</code>, <code>black</code>, <code>cloud</code>, <code>purple</code>, <code>grey</code>, <code>custom</code>, <code>llm</code></li> <li>Impact: Sets the main visual element of your README. Using <code>custom</code> allows you to specify your own image, while <code>llm</code> generates an image using AI.</li> </ul>"},{"location":"configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Start with defaults: Begin with the default options and gradually customize to find the best fit for your project.</li> <li>Match your project's style: Use badge and header styles that complement your project's branding or purpose.</li> <li>Balance detail and brevity: Adjust <code>tree-depth</code> based on your project's complexity. Deeper trees provide more detail but can make the README lengthy.</li> <li>Experiment with LLM settings: Try different combinations of <code>temperature</code> and <code>top-p</code> to find the right balance of creativity and coherence.</li> <li>Consider API usage: If using paid API services, be mindful of <code>rate-limit</code> and choose models that balance quality and cost.</li> </ol> <p>By carefully configuring these options, you can generate READMEs that are not only informative but also visually appealing and perfectly tailored to your project's needs.</p>"},{"location":"configuration/advanced-examples/","title":"Advanced Configurations (WIP)","text":"<p>The following examples demonstrate advanced configurations and options for generating READMEs with readme-ai.</p>"},{"location":"configuration/advanced-examples/#generate-readme-with-custom-openai-api-parameters","title":"Generate README with Custom OpenAI API Parameters","text":"<pre><code>readmeai --repository /path/to/project \\\n        --api openai \\\n        --model gpt-4-turbo\n        --context-window 9999\n        --temperature 0.1\n        --rate-limit 20\n</code></pre>"},{"location":"configuration/basic-examples/","title":"Example Configurations","text":"<p>This page provides simple examples of using <code>readme-ai</code> with different LLM services and basic configurations on the command line.</p>"},{"location":"configuration/basic-examples/#generate-readme-with-openai","title":"Generate README with OpenAI","text":"<pre><code>readmeai --repository https://github.com/username/project \\\n        --api openai \\\n        --model gpt-3.5-turbo\n</code></pre>"},{"location":"configuration/basic-examples/#generate-readme-with-ollama","title":"Generate README with Ollama","text":"<pre><code>readmeai --repository https://github.com/username/project \\\n        --api ollama \\\n        --model mistral\n</code></pre>"},{"location":"configuration/basic-examples/#generate-readme-with-google-gemini","title":"Generate README with Google Gemini","text":"<pre><code>readmeai --repository https://github.com/username/project \\\n        --api gemini \\\n        --model gemini-1.5-flash\n</code></pre>"},{"location":"configuration/basic-examples/#generate-readme-in-offline-mode","title":"Generate README in Offline Mode","text":"<pre><code>readmeai --repository https://github.com/username/project \\\n        --api offline\n</code></pre>"},{"location":"configuration/basic-examples/#customize-badge-style-and-color","title":"Customize Badge Style and Color","text":"<pre><code>readmeai --repository https://github.com/username/project \\\n        --api openai \\\n        --badge-style flat-square \\\n        --badge-color FF5733\n</code></pre>"},{"location":"configuration/basic-examples/#use-custom-project-logo","title":"Use Custom Project Logo","text":"<pre><code>readmeai --repository https://github.com/username/project \\\n        --api openai \\\n        --image custom\n</code></pre> <p>When prompted, enter the path or URL to your custom logo image.</p>"},{"location":"configuration/basic-examples/#generate-readme-with-emojis","title":"Generate README with Emojis","text":"<pre><code>readmeai --repository https://github.com/username/project \\\n        --api openai \\\n        --emojis\n</code></pre> <p>These examples demonstrate basic usage of readme-ai. For more advanced configurations and options, see the Advanced Configurations page.</p>"},{"location":"configuration/gallery/","title":"README Gallery","text":""},{"location":"configuration/gallery/#example-readme-files","title":"Example README Files","text":"Language/Framework Output File Input Repository Description Python readme-python.md readme-ai Core readme-ai project TypeScript &amp; React readme-typescript.md ChatGPT App React Native ChatGPT app PostgreSQL &amp; DuckDB readme-postgres.md Buenavista Postgres proxy server Kotlin &amp; Android readme-kotlin.md file.io Client Android file sharing app Streamlit readme-streamlit.md readme-ai-streamlit Streamlit UI for readme-ai app Rust &amp; C readme-rust-c.md CallMon System call monitoring tool Docker &amp; Go readme-go.md docker-gs-ping Dockerized Go app Java readme-java.md Minimal-Todo Minimalist todo Java app FastAPI &amp; Redis readme-fastapi-redis.md async-ml-inference Async ML inference service Jupyter Notebook readme-mlops.md mlops-course MLOps course repository Apache Flink readme-local.md Local Directory Example using a local directory <p><sub>See additional README file examples here</sub></p>"},{"location":"configuration/content/project-badges/","title":"Project Badges","text":""},{"location":"configuration/content/project-badges/#project-badges","title":"Project Badges","text":"<p>The <code>--badge-style</code> option lets you select the style of the default badge set.</p> Style Preview default flat flat-square for-the-badge plastic skills skills-light social <p>When providing the <code>--badge-style</code> option, readme-ai does two things:</p> <ol> <li>Formats the default badge set to match the selection (i.e. flat, flat-square, etc.).</li> <li>Generates an additional badge set representing your projects dependencies and tech stack (i.e. Python, Docker, etc.)</li> </ol>"},{"location":"configuration/content/project-badges/#example","title":"Example","text":"<p><code>sh \u276f readmeai --badge-style flat-square --repository https://github.com/eli64s/readme-ai</code> </p>"},{"location":"configuration/content/project-badges/#output","title":"Output","text":"<p>{... project logo ...}</p> <p>{... project name ...}</p> <p>{...project slogan...}</p> <p> </p> <p></p> <p>Developed with the software and tools below.</p> <p> </p> <p></p> <p>{... end of header ...} </p>"},{"location":"configuration/content/project-logo/","title":"Project Logo","text":""},{"location":"configuration/content/project-logo/#project-logo","title":"Project Logo","text":"<p>Select a project logo using the <code>--image</code> option.</p> blue gradient black cloud purple grey <p>For custom images, see the following options: - Use <code>--image custom</code> to invoke a prompt to upload a local image file path or URL. - Use <code>--image llm</code> to generate a project logo using a LLM API (OpenAI only).</p>"},{"location":"configuration/templates/headers/","title":"Header","text":""},{"location":"configuration/templates/headers/#headers","title":"Headers","text":"<ul> <li>WIP</li> </ul>"},{"location":"configuration/templates/table-of-contents/","title":"Table of Content","text":""},{"location":"configuration/templates/table-of-contents/#table-of-contents","title":"Table of Contents","text":"<ul> <li>WIP</li> </ul>"},{"location":"features/","title":"Key Features","text":"<p>The <code>readme-ai</code> tool offers a comprehensive set of features designed to streamline the creation of high-quality README.md files for your projects. Here's an overview of the key features:</p> <ol> <li> <p>Flexible README Generation: Combines robust repository context extraction with generative AI to create detailed and accurate README files.</p> </li> <li> <p>Customizable Output: Offers numerous CLI options for tailoring the README to your project's needs:</p> </li> <li>Badge styles and colors</li> <li>Header designs</li> <li>Table of contents styles</li> <li> <p>Project logos</p> </li> <li> <p>Language Agnostic: Works with a wide range of programming languages and project types, automatically detecting and summarizing key aspects of your codebase.</p> </li> <li> <p>Project Analysis: Automatically extracts and presents important information about your project:</p> </li> <li>Directory structure</li> <li>File summaries</li> <li>Dependencies</li> <li> <p>Setup instructions</p> </li> <li> <p>Multi-LLM Support: Compatible with various language model APIs, including:</p> </li> <li>OpenAI</li> <li>Ollama</li> <li>Google Gemini</li> <li> <p>Offline Mode (for generating boilerplate READMEs without API calls)</p> </li> <li> <p>Offline Mode: Generate a basic README structure without requiring an internet connection or API calls.</p> </li> <li> <p>Continuous Integration (WIP): Can be integrated into CI/CD pipelines for automated README generation and updates.</p> </li> </ol> <p>For more detailed information on each feature, please refer to the specific feature pages in the navigation menu.</p>"},{"location":"features/integrations/","title":"LLM API Integrations","text":"<p>Readme-ai integrates seamlessly with various Large Language Model (LLM) services to generate high-quality README content. This page provides an overview of the supported LLM services and links to detailed information about each.</p>"},{"location":"features/integrations/#supported-llm-services","title":"Supported LLM Services","text":"<ol> <li>OpenAI</li> <li>Ollama</li> <li>Google Gemini</li> <li>Offline Mode</li> </ol>"},{"location":"features/integrations/#comparing-llm-services","title":"Comparing LLM Services","text":"Service Pros Cons OpenAI High-quality output, Versatile Requires API key, Costs associated Ollama Free, Privacy-focused, Offline May be slower, Requires local setup Gemini Strong performance, Google integration Requires API key Offline No internet required, Fast Basic output, Limited customization"},{"location":"features/integrations/#tips-for-optimal-results","title":"Tips for Optimal Results","text":"<ol> <li>Experiment with different models: Try various LLM services and models to find the best fit for your project.</li> <li>Provide clear context: Ensure your repository has well-organized code and documentation to help the LLM generate more accurate content.</li> <li>Fine-tune with CLI options: Use readme-ai's CLI options to customize the output further after choosing your LLM service.</li> <li>Review and edit: Always review the generated README and make necessary edits to ensure accuracy and relevance to your project.</li> </ol> <p>By leveraging these LLM integrations effectively, you can generate comprehensive and accurate README files for your projects with minimal effort.</p>"},{"location":"features/integrations/google-gemini/","title":"Google Gemini Integration","text":"<p>Google's Gemini models offer strong performance across a wide range of tasks.</p>"},{"location":"features/integrations/google-gemini/#configuration","title":"Configuration","text":"<pre><code>readmeai --repository &lt;REPO_URL_OR_PATH&gt; --api gemini --model gemini-1.5-flash\n</code></pre>"},{"location":"features/integrations/google-gemini/#available-models","title":"Available Models","text":"<p>It is recommended to use the following models:</p> <ul> <li><code>gemini-1.5-pro</code></li> </ul>"},{"location":"features/integrations/google-gemini/#best-practices","title":"Best Practices","text":"<ul> <li>Gemini models excel at understanding context and generating coherent text.</li> <li>Ensure you have the necessary API credentials set up.</li> </ul>"},{"location":"features/integrations/offline-mode/","title":"Offline Mode","text":"<p>Offline mode generates a basic README structure without using any LLM service.</p>"},{"location":"features/integrations/offline-mode/#configuration","title":"Configuration","text":"<pre><code>readmeai --repository &lt;REPO_URL_OR_PATH&gt; --api offline\n</code></pre>"},{"location":"features/integrations/offline-mode/#best-practices","title":"Best Practices","text":"<ul> <li>Use offline mode for quick boilerplate generation or when you don't have internet access.</li> <li>Customize the generated README manually after generation.</li> </ul>"},{"location":"features/integrations/ollama/","title":"Ollama Integration","text":"<p>Ollama is a privacy-focused, open-source Large Language Model (LLM) that can be run locally to generate high-quality README content. Readme-ai integrates seamlessly with Ollama to provide a fast and privacy-friendly option for generating README files.</p> <p><sub>See more about Ollama on GitHub.</sub></p>"},{"location":"features/integrations/ollama/#configuration","title":"Configuration","text":"<pre><code>readmeai --repository &lt;REPO_URL_OR_PATH&gt; --api ollama --model llama3\n</code></pre>"},{"location":"features/integrations/ollama/#available-models","title":"Available Models","text":"<p>You can pull any model from the Ollama repository. Some recommended models include: - <code>llama2</code> - <code>mistral</code> - <code>gemma2</code></p>"},{"location":"features/integrations/ollama/#best-practices","title":"Best Practices","text":"<ul> <li>Ensure Ollama is running locally before using this option.</li> <li>Ollama models run offline, providing privacy and speed benefits.</li> </ul>"},{"location":"features/integrations/openai/","title":"OpenAI Integration","text":"<p>OpenAI's GPT models are known for their versatility and high-quality text generation.</p>"},{"location":"features/integrations/openai/#configuration","title":"Configuration","text":"<pre><code>readmeai --repository &lt;REPO_URL_OR_PATH&gt; --api openai --model gpt-3.5-turbo\n</code></pre>"},{"location":"features/integrations/openai/#available-models","title":"Available Models","text":"<p>Although not limited to the following, it is recommended to use the following models: - <code>gpt-3.5-turbo</code> - <code>gpt-4</code> - <code>gpt-4-turbo</code></p>"},{"location":"features/integrations/openai/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>gpt-3.5-turbo</code> for faster generation and lower costs.</li> <li>Use <code>gpt-4</code> or <code>gpt-4-turbo</code> for more complex projects or when you need higher accuracy.</li> </ul>"},{"location":"quickstart/installation/","title":"Installation","text":"<p>Install <code>readmeai</code> using one of the following methods:</p>"},{"location":"quickstart/installation/#pip","title":"Pip","text":"<pre><code>pip install readmeai\n</code></pre>"},{"location":"quickstart/installation/#pipx","title":"Pipx","text":"<p>pipx is recommended for installing Python CLI applications:</p> <pre><code>pipx install readmeai\n</code></pre>"},{"location":"quickstart/installation/#docker","title":"Docker","text":"<pre><code>docker pull zeroxeli/readme-ai:latest\n</code></pre>"},{"location":"quickstart/installation/#from-source","title":"From Source","text":"<ol> <li> <p>Clone the repository:    <code>sh    git clone https://github.com/eli64s/readme-ai    cd readme-ai</code></p> </li> <li> <p>Install using one of the following methods:</p> </li> </ol> <p>Using bash:    <code>sh    bash setup/setup.sh</code></p> <p>Using poetry:    <code>sh    poetry install</code></p> <p>After installation, verify that readme-ai is correctly installed by running:</p> <pre><code>readmeai --version\n</code></pre> <p>For usage instructions, see the Usage documentation.</p>"},{"location":"quickstart/requirements/","title":"Quickstart","text":"<p>Please ensure that you have the following prerequisites installed and resources configured before proceeding with the quickstart guide.</p>"},{"location":"quickstart/requirements/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: <code>3.9</code> or higher.</li> <li>Package Manage/Container: <code>pip</code>, <code>pipx</code>, or <code>docker</code>.</li> </ul>"},{"location":"quickstart/requirements/#git-repository-or-local-directory","title":"Git Repository or Local Directory","text":"<p>A Git repository or a local file system directory is required to generate README files. Supported platforms include:</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>Bitbucket</li> <li>File System</li> </ul> <p><sub>If your Git provider is not listed, open an issue or submit a pull request to add support for additional platforms.</sub></p>"},{"location":"quickstart/requirements/#llm-api-key","title":"LLM API Key","text":"<p>To enable the full functionality of <code>readmeai</code>, an API key from one of the following providers is required:</p> <ul> <li>OpenAI: OpenAI API</li> <li>Ollama: Ollama API</li> <li>Google Gemini: Google Gemini API</li> <li>Offline Mode: No API key required.</li> </ul> <p><sub>For more information on setting up API keys, refer to the specific documentation for each API provider Additionally, support for <code>Anthropic</code> and <code>Litellm</code> is planned for future releases.</sub></p>"},{"location":"quickstart/usage/cli/","title":"CLI Usage","text":"<p>This guide covers the basic usage of readme-ai and provides examples for different LLM services.</p>"},{"location":"quickstart/usage/cli/#basic-usage","title":"Basic Usage","text":"<p>The general syntax for using readme-ai is:</p> <pre><code>readmeai --repository &lt;REPO_URL_OR_PATH&gt; --api &lt;LLM_SERVICE&gt; [OPTIONS]\n</code></pre> <p>Replace <code>&lt;REPO_URL_OR_PATH&gt;</code> with your repository URL or local path, and <code>&lt;LLM_SERVICE&gt;</code> with your chosen LLM service (openai, ollama, gemini, or offline).</p>"},{"location":"quickstart/usage/cli/#examples","title":"Examples","text":""},{"location":"quickstart/usage/cli/#using-openai","title":"Using OpenAI","text":"<pre><code>readmeai --repository https://github.com/eli64s/readme-ai \\\n         --api openai \\\n         --model gpt-3.5-turbo\n</code></pre>"},{"location":"quickstart/usage/cli/#using-ollama","title":"Using Ollama","text":"<pre><code>readmeai --repository https://github.com/eli64s/readme-ai \\\n         --api ollama \\\n         --model llama3\n</code></pre>"},{"location":"quickstart/usage/cli/#using-google-gemini","title":"Using Google Gemini","text":"<pre><code>readmeai --repository https://github.com/eli64s/readme-ai \\\n         --api gemini \\\n         --model gemini-1.5-flash\n</code></pre>"},{"location":"quickstart/usage/cli/#offline-mode","title":"Offline Mode","text":"<pre><code>readmeai --repository https://github.com/eli64s/readme-ai \\\n         --api offline\n</code></pre>"},{"location":"quickstart/usage/cli/#advanced-usage","title":"Advanced Usage","text":"<p>You can customize the output using various options:</p> <pre><code>readmeai --repository https://github.com/eli64s/readme-ai \\\n         --output readmeai.md \\\n         --api openai \\\n         --model gpt-4-turbo \\\n         --badge-color A931EC \\\n         --badge-style flat-square \\\n         --header-style compact \\\n         --toc-style fold \\\n         --temperature 0.1 \\\n         --tree-depth 2 \\\n         --image LLM \\\n         --emojis\n</code></pre> <p>For a full list of options, run:</p> <pre><code>readmeai --help\n</code></pre> <p>See the Configuration Options documentation for detailed explanations of each option.</p>"},{"location":"quickstart/usage/cli/#tips-for-effective-usage","title":"Tips for Effective Usage","text":"<ol> <li>Choose the right LLM: Different LLMs may produce varying results. Experiment to find the best fit for your project.</li> <li>Adjust temperature: Lower values (e.g., 0.1) produce more focused output, while higher values (e.g., 0.8) increase creativity.</li> <li>Use custom prompts: For specialized projects, consider using custom prompts to guide the AI's output.</li> <li>Review and edit: Always review the generated README and make necessary adjustments to ensure accuracy and relevance.</li> </ol>"},{"location":"quickstart/usage/cli/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues:</p> <ol> <li>Ensure you have the latest version of readme-ai installed.</li> <li>Check your API credentials if using OpenAI or Google Gemini.</li> <li>For Ollama, make sure the Ollama service is running locally.</li> <li>Consult the FAQ or open an issue for additional support.</li> </ol>"},{"location":"quickstart/usage/docker/","title":"Running with Docker","text":"<p>To run <code>readme-ai</code> in a Docker container, you can either pull the latest Docker image from Docker Hub or build the Docker image from the source code.</p>"},{"location":"quickstart/usage/docker/#pull-the-docker-image","title":"Pull the Docker Image","text":"<p>Pull the latest <code>readme-ai</code> Docker image from Docker Hub using the following command:</p> <pre><code>docker pull zeroxeli/readme-ai:latest\n</code></pre>"},{"location":"quickstart/usage/docker/#run-the-docker-container","title":"Run the Docker Container","text":"<p>Run the <code>readme-ai</code> Docker container using the following command:</p> <pre><code>docker run -it \\\n        -e OPENAI_API_KEY=$OPENAI_API_KEY \\\n        -v \"$(pwd)\":/app zeroxeli/readme-ai:latest \\\n        -r https://github.com/eli64s/readme-ai\n</code></pre>"},{"location":"quickstart/usage/streamlit/","title":"Running on Streamlit Cloud","text":"<p>Run <code>readme-ai</code> directly in your browser using Streamlit Cloud. No installation required!</p> <p></p> <p>For more details about the implementation, see the readme-ai-streamlit repository on GitHub.</p>"}]}